{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob\n",
    "import os.path \n",
    "from __future__ import division\n",
    "import math\n",
    "import re\n",
    "\n",
    "\n",
    "def matching( f ):\n",
    "    x=f.name.rsplit('\\\\')\n",
    "#     print(x)\n",
    "    y=x[2].rsplit('.')\n",
    "#     print(y)\n",
    "    match = re.match(r\"([a-z]+)([0-9]+)\", y[0], re.I)\n",
    "    if match:\n",
    "         items = match.groups()\n",
    "    return items\n",
    "\n",
    "\n",
    "def big_doc_c(replace):\n",
    "        list_of_files = glob.glob('corpus\\\\'+replace+'\\\\train\\\\*.txt') \n",
    "        i=0\n",
    "\n",
    "        for file_name in list_of_files:\n",
    "            f2write = open('corpus\\\\bigdoc_c\\\\'+replace+'%s.txt' % i,'w')\n",
    "            i+=1\n",
    "            with open(file_name,'r') as f:\n",
    "                for line in f:\n",
    "                    if(line!='\\n'):\n",
    "                        for word in line.split():\n",
    "                            f2write.write(word+\"\\n\")\n",
    "            f2write.close()\n",
    "def testdoc(replace):\n",
    "    list_of_files = glob.glob('corpus\\\\'+replace+'\\\\test\\\\*.txt') \n",
    "    i=0\n",
    "    for file_name in list_of_files:\n",
    "        f2write = open('corpus\\\\test_complete_doc\\\\'+replace+'%s.txt' % i,'w')\n",
    "        i+=1\n",
    "        with open(file_name,'r') as f:\n",
    "            for line in f:\n",
    "                if(line!='\\n'):\n",
    "#                 f1=open(\"unigram_tokens_of_english.txt\",\"a\")\n",
    "                    for word in line.split():\n",
    "#                 word_list.append(word)\n",
    "                        f2write.write(word+\"\\n\")\n",
    "        f2write.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab(replace):\n",
    "        value = 1\n",
    "        new_dict_words = []\n",
    "        total_count_distinct_words = 0\n",
    "        dict_word = []\n",
    "        \n",
    "        \n",
    "#         tokens in files are genareted\n",
    "        \n",
    "        \n",
    "        list_of_files = glob.glob('corpus\\\\'+replace+'\\\\train\\\\*.txt') \n",
    "        i=0\n",
    "        for file_name in list_of_files:\n",
    "            f2write_pve = open('corpus\\\\big_doc_D\\\\'+replace+'%s.txt' % i,'w')\n",
    "            i+=1\n",
    "            with open(file_name,'r') as f:\n",
    "                for line in f:\n",
    "                    if(line!='\\n'):\n",
    "                        for word in line.split():\n",
    "                            f2write_pve.write(word+\"\\n\")\n",
    "            f2write_pve.close()\n",
    "        \n",
    "#     for creating word-LIST for making dictionary\n",
    "    \n",
    "        word_seq = []\n",
    "        counts = []\n",
    "        list_of_files = glob.glob('corpus\\\\big_doc_D\\\\*.txt') \n",
    "        for filename in (list_of_files):\n",
    "            with open(filename,'r') as f:  \n",
    "                for word in f:\n",
    "                    dict_word.append(word)\n",
    "        \n",
    "        ii=0\n",
    "        while ii < len(dict_word):\n",
    "            word = dict_word[ii]\n",
    "            i=0\n",
    "            while i < len(new_dict_words):\n",
    "                if(new_dict_words[i]==word):\n",
    "                    value=0\n",
    "                    break\n",
    "                else:\n",
    "                    value = 1\n",
    "                i+=1\n",
    "            if(value==1):\n",
    "                new_dict_words.append(word)\n",
    "                total_count_distinct_words+=1\n",
    "            ii+=1\n",
    "        print(\"Total number of vocab elements : \"+str(total_count_distinct_words))\n",
    "        return new_dict_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of vocab elements : 51\n",
      "Total number of vocab elements : 51\n",
      "Tokens Count : 35\n",
      "Vocab Length : 51\n",
      "0\n",
      "Tokens Count : 29\n",
      "Vocab Length : 51\n",
      "1\n",
      "Prior: [0.5, 0.5]\n",
      "Iqbal Livelihood: \n",
      "[0.011627906976744186, 0.011627906976744186, 0.023255813953488372, 0.011627906976744186, 0.03488372093023256, 0.011627906976744186, 0.011627906976744186, 0.046511627906976744, 0.011627906976744186, 0.011627906976744186, 0.011627906976744186, 0.011627906976744186, 0.011627906976744186, 0.011627906976744186, 0.011627906976744186, 0.011627906976744186, 0.011627906976744186, 0.011627906976744186, 0.011627906976744186, 0.011627906976744186, 0.011627906976744186, 0.011627906976744186, 0.011627906976744186, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.03488372093023256, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372]\n",
      "Ghalib Livelihood: \n",
      "[0.025, 0.025, 0.0625, 0.025, 0.025, 0.0375, 0.025, 0.05, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n"
     ]
    }
   ],
   "source": [
    "ijk=0\n",
    "\n",
    "word_asad = []\n",
    "word_ghalib = []\n",
    "word_iqbal = []\n",
    "\n",
    "total_count_asad=0\n",
    "total_count_iqbal=0\n",
    "total_count_ghalib=0\n",
    "\n",
    "asad_livelihood=[]\n",
    "iqbal_livelihood=[]\n",
    "ghalib_livelihood=[]\n",
    "\n",
    "prior = []\n",
    "\n",
    "\n",
    "V = []\n",
    "\n",
    "iqbal_files = glob.glob('corpus\\\\iqbal\\\\train\\\\*.txt')  \n",
    "ghalib_files = glob.glob('corpus\\\\ghalib\\\\train\\\\*.txt')\n",
    "\n",
    "total_files = len(iqbal_files) + len(ghalib_files)\n",
    "ijk=0\n",
    "while(ijk<2):\n",
    "\n",
    "    if(ijk==0):\n",
    "        replace = \"iqbal\"\n",
    "    if(ijk==1):\n",
    "        replace = \"ghalib\"\n",
    "#     prior\n",
    "    files = glob.glob('corpus\\\\'+replace+'\\\\train\\\\*.txt') \n",
    "    prior.append((float(len(files)/total_files)))\n",
    "#     vocab\n",
    "    V = vocab(replace)\n",
    "#     big_document\n",
    "    big_doc_c(replace)\n",
    "\n",
    "    \n",
    "    ijk+=1\n",
    "ijk=0\n",
    "while(ijk<2):\n",
    "\n",
    "\n",
    "    if(ijk==0):\n",
    "        replace = \"iqbal\"\n",
    "    if(ijk==1):\n",
    "        replace = \"ghalib\"\n",
    "        \n",
    "#     tokens count in documents\n",
    "\n",
    "    tokens_count = 0\n",
    "    list_of_files1  = glob.glob('corpus\\\\bigdoc_c\\\\*.txt') \n",
    "    for file_name1 in list_of_files1:\n",
    "            with open(file_name1,'r') as f1: \n",
    "                items = matching(f1)\n",
    "                if(items[0]==replace):\n",
    "                    for line1 in f1:\n",
    "                        tokens_count = tokens_count + 1\n",
    "    print(\"Tokens Count : \"+str(tokens_count))\n",
    "\n",
    "#      Calculating Livelihood and appending word sequence \n",
    "    print(\"Vocab Length : \"+str(len(V)))\n",
    "    for w in V:\n",
    "                    list_of_files1  = glob.glob('corpus\\\\bigdoc_c\\\\*.txt') \n",
    "                    c=0\n",
    "                    for file_name1 in list_of_files1:\n",
    "                        with open(file_name1,'r') as f1: \n",
    "                                        items = matching(f1)\n",
    "\n",
    "                                        if(items[0]==replace):\n",
    "                                            for line1 in f1:\n",
    "                                                if(w==line1):\n",
    "                                                    c = c + 1\n",
    "            \n",
    "            \n",
    "                    if(replace==\"iqbal\"):\n",
    "                        word_iqbal.append(w)\n",
    "                        iqbal_livelihood.append((float((c+1)/((tokens_count)+len(V)))))\n",
    "                    if(replace==\"ghalib\"):\n",
    "                        word_ghalib.append(w)\n",
    "                        ghalib_livelihood.append((float((c+1)/((tokens_count)+len(V)))))\n",
    "    \n",
    "    print(ijk)    \n",
    "    ijk+=1\n",
    "# print(pve_livelihood)\n",
    "# print(nve_livelihood)\n",
    "# print(word_pve)\n",
    "# print(word_nve)\n",
    "# print(len(V))\n",
    "print(\"Prior: \"+str(prior))\n",
    "print(\"Iqbal Livelihood: \")\n",
    "print(iqbal_livelihood)\n",
    "print(\"Ghalib Livelihood: \")\n",
    "print(ghalib_livelihood)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ijk=0\n",
    "while(ijk<2):\n",
    "    if(ijk==0):\n",
    "        replace = \"iqbal\"\n",
    "    if(ijk==1):\n",
    "        replace = \"ghalib\"\n",
    "    testdoc(replace)\n",
    "    ijk+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n",
      "Accuracy: 33.3333333333\n",
      "Precision: 50.0\n",
      "Recall: 50.0\n",
      "         True,   False\n",
      "Positive   2   0\n",
      "Negative   2   2\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "#              \t\tCaculating Contigency Matrix\n",
    "#################################################################################\n",
    "from __future__ import division\n",
    "import math\n",
    "i=0\n",
    "j=2\n",
    "tn=0\n",
    "tp=0\n",
    "fp=0\n",
    "fn=0\n",
    "sum = 1\n",
    "sum_concat = []\n",
    "import re \n",
    "list_of_files = glob.glob('corpus\\\\test_complete_doc\\\\*.txt') \n",
    "i=0\n",
    "j=3\n",
    "b=1\n",
    "c=1\n",
    "\n",
    "\n",
    "print(prior[0])\n",
    "print(prior[1])\n",
    "for file_name in list_of_files:\n",
    "        sum_concat = []\n",
    "\n",
    "        i=0\n",
    "        while i < 3:\n",
    "            if(i==0):\n",
    "                sum=(sum*prior[0])\n",
    "            elif(i==1):\n",
    "                sum=(sum*prior[1])\n",
    "            with open(file_name,'r') as f:\n",
    "                      for line in f:\n",
    "                        for words in line.split():\n",
    "                            if (words+'\\n') in V:\n",
    "                                    if(i==0):\n",
    "                                        if (words+'\\n') in word_iqbal:\n",
    "                                                x=word_iqbal.index(words+'\\n')\n",
    "                                                sum=(sum*iqbal_livelihood[x])\n",
    "                                    if(i==1):\n",
    "                                        if (words+'\\n') in word_ghalib:\n",
    "                                                x=word_ghalib.index(words+'\\n')\n",
    "                                                sum=float(sum*ghalib_livelihood[x])\n",
    "            \n",
    "            sum_concat.append(sum)\n",
    "            i+=1\n",
    "            sum=1\n",
    "#             print((sum_concat))\n",
    "            \n",
    "            if(len(sum_concat)>1):\n",
    "                items = matching(f)\n",
    "                if(sum_concat[1]<sum_concat[0]):\n",
    "                    if(items[0]==\"iqbal\"):\n",
    "                        tp+=1\n",
    "                    elif(items[0]==\"ghalib\"):\n",
    "                        tn+=1\n",
    "                elif(sum_concat[0]<sum_concat[1]):\n",
    "                    if(items[0]==\"iqbal\"):\n",
    "                        fp+=1\n",
    "                    elif(items[0]==\"ghalib\"):\n",
    "                        fn+=1\n",
    "                \n",
    "# recall=float((tp)/(tp+fn))\n",
    "# precision=float((tp)/(tp+tn))\n",
    "recall=float((tp)/(tp+fn))\n",
    "precision=float((tp)/(tp+fp))\n",
    "accuracy=float((tp+tn)/(tp+tn+fp+fn))\n",
    "print(\"Accuracy: \"+str(accuracy*100))\n",
    "print(\"Precision: \"+str(precision*100))\n",
    "print(\"Recall: \"+str(recall*100))\n",
    "print(\"         True,   False\")\n",
    "print(\"Positive   \"+str(tp)+\"   \"+str(tn))\n",
    "print(\"Negative   \"+str(fp)+\"   \"+str(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
